# Lameness Detection System - Environment Configuration
# Copy this file to .env and modify as needed
# Usage: cp env.example .env

# ============== HOST CONFIGURATION ==============
# The hostname/IP where services are accessible
DEPLOY_HOST=localhost

# ============== SERVICE PORTS ==============
POSTGRES_PORT=5432
NATS_PORT=4222
QDRANT_PORT=6333
BACKEND_PORT=8000
FRONTEND_PORT=3000

# ============== DATABASE ==============
POSTGRES_USER=lameness_user
POSTGRES_DB=lameness_db
POSTGRES_PASSWORD=lameness_pass

# ============== QDRANT ==============
# Embedding vector size (768 for DINOv3)
QDRANT_EMBEDDING_SIZE=768

# ============== NATS ==============
NATS_URL=nats://nats:4222

# ============== LLM CONFIGURATION ==============
# Priority: OpenAI > Ollama Local > Skip
# Set OPENAI_API_KEY to use OpenAI (preferred)
# OPENAI_API_KEY=sk-your-openai-key-here
# OPENAI_MODEL=gpt-4o-mini

# Ollama Local LLM (fallback if no OpenAI key)
# OLLAMA_HOST=http://host.docker.internal:11434
# OLLAMA_MODEL=llama3.2
# Recommended models: llama3.2, mistral, phi3, gemma2

# ============== API CONFIGURATION ==============
# For production, set your actual domain
# VITE_API_URL=https://api.yourdomain.com

